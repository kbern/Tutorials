{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first thing we need to do, as always, is to import the correct libraries. We will not be using all of these libraries while extracting the data, but we will need to use them all eventually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from datetime import *\n",
    "from dateutil.relativedelta import *\n",
    "from sklearn.preprocessing import normalize\n",
    "from time import time\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction import text \n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can now access our data by placing our desired csv file in an s3 bucket on AWS and downloading the file from that bucket. Then we will read in the file and save it as a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'data-science-tutorials'\n",
    "key = 'topic_modeling.csv'\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "s3.Bucket(bucket).download_file(key,key)\n",
    "\n",
    "df = pd.read_csv('./topic_modeling.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So what exactly is topic modeling? Topic modeling is a process of discovering the abstract \"topics\" that occur in a collection of documents. TF-IDF,  used as a weighting factor in topic modeling, stands for term frequency-Inverse document frequency. It is a statistic that reflects how important a word is in a corpus. This is used to weight different words so that a model can (narrow down the word search when looking for which words are most important in a set of words). In this particular example, we will be applying an NMF model to our TF-IDF An NMF (Non-negative matrix factorization) model, included in the Skicit Learn library, is a technique used for topic modeling that finds topics in a text based on correlations using linear algebra. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we can create a variable *n_top_words* that specifies the number of 'top' words we will be printing out for each topic. We also have the variable *no_topics* to specify the number of topics. In our case, we will be configuring 5 topics each with 5 top words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 5\n",
    "no_topics = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we will create a function *get_topics* that creates a dictionary that displays the topic key as well as the topic words for each key and adds it to a dataframe. Once it is in a dataframe, we can easily drop duplicate topics.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics_df(model, feature_names, n_top_words):\n",
    "    topic_index = []\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        topic_list = ([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        topic_list = \" \".join(topic_list)\n",
    "        topic_index.append(message)\n",
    "        topics.append(topic_list)\n",
    "    d = {'topic_index': topic_index, 'topics': topics}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    df.drop_duplicates(subset='topics', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the vectorizor, there are many different parameters we can implement in order to adjust the accuracy of the model. One parameter used is *stop_words*. This is a function that filters out useless word data such as “the”, “a”, “an”, “in”. To increase the accuracy of our model, we can add our own stop words so that we can make sure not to include words that provide no meaning to our model. We should get rid of words such as \"hello\", \"yes\", \"bye\" etc since they do not provide any insight into what the calls are about. Other parameters used were *max_features* which is set to *no_features*. *min_df* and *max_df* adjust the cutoffs for the words examined in the dataset. For example, and min_df of .05 mean that for the words to be examined, it must appear in at least 5 percent of the calls. A max_df of .7 means that any words that appear in over 70 percent of the calls will not be used. This is necessary because if a word appears too little or too often then it loses importance.  In this case, a very low *max_df* makes it a lot easier to extract more unique, meaningful words. Next we will fit and transform the vectorizor with adjusted parameters onto our transcript data. (tokenize and count the word occurrences of a corpus of transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n",
      "done in 0.560s. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "my_added_stop_words = ['yeah','yes','hey','hi','good','bye','like',]\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(my_added_stop_words)\n",
    "tfidf_vectorizer = TfidfVectorizer( max_df=0.30, min_df=0.02, stop_words = stop_words)\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit(df['transcript'])\n",
    "tfidf_vector = tfidf.transform(df['transcript'])\n",
    "print(\"done in %0.3fs.\" % (time() - t0), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can now start fitting the NMF model using the tfidf features. Here, we will adjust the parameters of the NMF model according to the 'Frobenius norm' model. We will then fit the model to the already transformed vectorizor. Now we can use the sklearn function *get_feature_names* which will receive all of the words in the vectorizor. We will then apply the nmf model, feature names, and n_top_words to the get_topics_df function which will return a dataframe of the topics as well as the top words for each topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model (Frobenius norm) with tf-idf features\n",
      "done in 0.471s.\n",
      "\n",
      "Topics in NMF model (Frobenius norm):\n",
      "  topic_index                                          topics\n",
      "0  Topic #0:                    account phone alright don let\n",
      "1  Topic #1:                  seven record tone finished hang\n",
      "2  Topic #2:                extension dial party person enter\n",
      "3  Topic #3:               leave reached soon possible return\n",
      "4  Topic #4:   representative speak customer support existing \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the NMF model with Frobenius norm with tf-idf features\n",
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features\")\n",
    "t0 = time()\n",
    "nmf_F = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.7, init='nndsvd').fit(tfidf_vector)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "topics_df = get_topics_df(nmf_F, tfidf_feature_names, n_top_words)\n",
    "print(topics_df,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As you can see, the words for each topic are not extremely cohesive, but can be genuinely distinguished by categories relating to voicemail, help, sales, marketing, etc. By raising the number of calls we use from 1,000 to 100,000, we should see an improvement, but it would take much longer to run. We could also improve the top words by adjusting the parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have a visual and qualitative measure of our data, we can work on getting quantitative data. We can quantitatively measure the scores for each call as well as the difference between each call using cosine similarity. By fitting and transforming the nmf model to the transformed vectorizor, we could get a matrix for each call that has a score for each of the topics. Instead of just printing out a list of matrices, we can write a code that compares the scores of the first call to the scores of all the other calls by comparing matrices to one another. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order to do this, we first must initialize call number 1 so we have a call to compare the rest of the calls to. Call 1 needs to be in the form of a matrix rather than a transcript so we need to vectorize the transcript by transforming the call to the fitted transcript. Then, we can transform the nmf model to the initial call 1 transcript. Now we will create a dataframe that includes the cosine similarities as well as the call ID for each call we are comparing Call 1 to. To compare call 1 to all other calls, we will create a for loop that vectorizes the transcript and applies the nmf model, same as what we did for Call 1.  Then, we can use the sklearn function *cosine_similarity*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get scores\n",
    "nmf_F.fit(tfidf_vector)\n",
    "\n",
    "#compare the scores of the first call to the scores of the other calls using cosine similarity\n",
    "#create a dataframe with the call ID and the cosine similarity for each call\n",
    "#sort in ascending order by cosine similarity\n",
    "call1 = df['transcript'][0]\n",
    "vect_0 = tfidf.transform([call1]) #vectorize transcript\n",
    "call1 = nmf_F.transform(vect_0)\n",
    "F_df = pd.DataFrame(columns=['sid','cos_sim'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can also find the scores for all the topics in each call by fitting the tfidf vectorizer trascriptions to the nmf model. Then we can transform the tfidf onto the nmf model which will show a matrix for each call that has 10 numbers in it for each of the topics. With these matrices we can preform something called 'cosine similarity' on each of the calls. Cosine similariy calculates the cosine of the angle between the two vectors. In this case, each vector is the matrix for each call, so we can just calculate the cosine similary by looping through the calls and comparing one call to all of the others. The smaller the number is, the more similar one call is to the next. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                sid   cos_sim\n",
      "0  180727c0b478f3a1  1.000000\n",
      "0  1807302890ecfa47  0.999905\n",
      "0  180724d9e2665237  0.999892\n",
      "0  180726b27f0fa518  0.999687\n",
      "0  1807273536a761f3  0.999659 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index,row in df.iterrows():\n",
    "    vect = tfidf.transform([row['transcript']]) #vectorize transcript\n",
    "    current_call = nmf_F.transform(vect) #nmf_KL.transform(tfidf) on single transcript\n",
    "    cos_sim = cosine_similarity(call1, current_call) #topics matrix --> cosine distance\n",
    "    sid = row['sid']\n",
    "    cos_sim = float(cos_sim[0])\n",
    "    F_df = F_df.append(pd.DataFrame({'sid': [sid], 'cos_sim': [cos_sim]}))\n",
    "F_df = F_df.sort_values(by='cos_sim', ascending=False)\n",
    "F_df.to_csv('./Ftest.csv')\n",
    "print(F_df.head(),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can now do all of the same steps, but on a new NMF model with different parameters. In this case, we will adjust the parameters according to the 'Kullback-Leibler divergence' model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model (generalized Kullback-Leibler divergence)\n",
      "done in 0.964s.\n",
      "\n",
      "Topics in NMF model (generalized Kullback-Leibler divergence):\n",
      "  topic_index                                 topics\n",
      "0  Topic #0:           alright mail thanks phone don\n",
      "1  Topic #1:       tone available record seven pound\n",
      "2  Topic #2:        extension party dial enter reach\n",
      "3  Topic #3:      leave reached soon marketing right\n",
      "4  Topic #4:   recorded speak maybe quality customer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/nltk_practice/lib/python3.6/site-packages/sklearn/decomposition/nmf.py:806: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if (previous_error - error) / error_at_init < tol:\n",
      "/anaconda3/envs/nltk_practice/lib/python3.6/site-packages/sklearn/decomposition/nmf.py:1035: ConvergenceWarning: Maximum number of iteration 1000 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                sid   cos_sim\n",
      "0  180727c0b478f3a1  1.000000\n",
      "0  180725e9dc42a489  0.999895\n",
      "0  180725477e9fbdee  0.999780\n",
      "0  180726f4df694ecd  0.999689\n",
      "0  180725fc2ef52735  0.999614\n"
     ]
    }
   ],
   "source": [
    "# Fit the NMF model with Kullback-Leibler divergence\n",
    "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence)\")\n",
    "t0 = time()\n",
    "nmf_KL = NMF(n_components=no_topics, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=0,\n",
    "          l1_ratio=.5).fit(tfidf_vector)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "topics_df = get_topics_df(nmf_KL, tfidf_feature_names, n_top_words)\n",
    "print(topics_df)\n",
    "\n",
    "#get scores\n",
    "nmf_KL.fit(tfidf_vector)\n",
    "\n",
    "#compare the scores of the first call to the scores of the other calls using cosine similarity\n",
    "#create a dataframe with the call ID and the cosine similarity for each call\n",
    "#sort in ascending order by cosine similarity\n",
    "\n",
    "call1 = df['transcript'][0]\n",
    "vect_0 = tfidf.transform([call1]) \n",
    "call1 = nmf_KL.transform(vect_0)\n",
    "KL_df = pd.DataFrame(columns=['sid','cos_sim'])\n",
    "for index,row in df.iterrows():\n",
    "    vect = tfidf.transform([row['transcript']]) #vectorize transcript\n",
    "    current_call = nmf_KL.transform(vect) #nmf_KL.transform(tfidf) on single transcript\n",
    "    cos_sim = cosine_similarity(call1, current_call) #topics matrix --> cosine distance\n",
    "    sid = row['sid']\n",
    "    cos_sim = float(cos_sim[0])\n",
    "    KL_df = KL_df.append(pd.DataFrame({'sid': [sid], 'cos_sim': [cos_sim]}))\n",
    "KL_df = KL_df.sort_values(by='cos_sim', ascending=False)\n",
    "KL_df.to_csv('./KLtest.csv')\n",
    "print(KL_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can open up the files KL_df and F_df in order to see the cosine similarities for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall, when choosing the number of topics and evaluating the interpretability of your topic model, it is important to look at both qualitative and quantitative factors. The cosine similarities are just one quantitative measure of topic modeling. In a Dialogtech hypothetical use case of topic modeling on phone call transcripts, it’s quantitatively useful to have topic categories to classify a call and understand general trends. It’s also qualitatively useful to drill into those topics and understand the nuances of each caller's individual requests.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
